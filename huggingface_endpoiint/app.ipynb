{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --quiet huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/charan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "repo_id = \"google/gemma-7b\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, max_new_tokens=512, temperature=0.5, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the FIFA World Cup in the year 2022? \"\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/charan/VS_Code/GitHub/Advanced_RAG/venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "1. In the year 2022, the FIFA World Cup was held in Qatar. \n",
      "\n",
      "2. The final match was played between Argentina and France. \n",
      "\n",
      "3. Argentina won the match and the FIFA World Cup in the year 2022. \n",
      "\n",
      "Therefore, the answer to the question is Argentina. \n",
      "\n",
      "The FIFA World Cup is the biggest and most prestigious football tournament in the world. It is held every four years and features the best teams from around the globe. The 2022 edition of the tournament was held in Qatar and was won by Argentina, who defeated France in a thrilling final. The tournament was won by Argentina, who defeated France in a thrilling final. The final match was played at the Lusail Stadium in Doha, Qatar, and was watched by millions of fans around the world. \n",
      "\n",
      "The final match was a back-and-forth affair, with both teams trading goals and chances. In the end, it was Argentina who emerged victorious, with a score of 4-2 after extra time. The victory was especially sweet for Argentina, who had lost the final in 2014 to Germany. \n",
      "\n",
      "The 2022 FIFA World Cup was a historic tournament, with many memorable moments. From the thrilling final to the incredible goals scored throughout the tournament, the World Cup was a true spectacle. Argentina's victory was a testament to their hard work and dedication, and they will be remembered as one of the greatest teams in World Cup history.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "# Load blog post\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# VectorDB\n",
    "embedding = HuggingFaceBgeEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nJay loves yellow but not red\\n\\nJack loves orange but not blue\\n\\nJaden loves blue but not red\\n\\nJosh loves red but not blue\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not red\\n\\nJames loves red but not green\\n\\nJames loves blue but not orange\\n\\nJosh loves blue but not red\\n\\nJamal loves yellow but not as much as he loves orange\\n\\nJack loves blue but not orange\\n\\nJaden loves green but not as much as he loves orange\\n\\nJosh loves green but not as much as he loves orange\\n\\nJohnny loves blue but not green\\n\\nJoe loves green but not as much as he loves orange\\n\\nJesse loves blue but not red\\n\\nJamal loves green but not as much as he loves orange\\n\\nJosh loves orange but not blue\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves blue but not green\\n\\nJesse loves blue but not green\\n\\nJosh loves orange but not red\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves orange but not green\\n\\nJosh loves orange but not green\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves orange but not green\\n\\nJosh loves orange but not green\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves orange but not green\\n\\nJosh loves orange but not green\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves orange but not green\\n\\nJosh loves orange but not green\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves orange but not green\\n\\nJosh loves orange but not green\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves orange but not green\\n\\nJosh loves orange but not green\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves orange but not green\\n\\nJosh loves orange but not green\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves orange but not green\\n\\nJosh loves orange but not green\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves orange but not green\\n\\nJosh loves orange but not green\\n\\nJohnny loves green but not as much as he loves orange\\n\\nJoe loves orange but not green\\n\\nJesse loves'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"What are everyone's favorite colors:\\n\\n{context}\")]\n",
    ")\n",
    "\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"Jesse loves red but not yellow\"),\n",
    "    Document(page_content = \"Jamal loves green but not as much as he loves orange\")\n",
    "]\n",
    "\n",
    "chain.invoke({\"context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "retrievalQA = RetrievalQA.from_llm(llm=llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'LLM Powered Autonomous Agents?',\n",
       " 'result': ' The answer is yes.\\nUnhelpful Answer: The answer is no.\\nUnhelpful Answer: The answer is maybe.\\nUnhelpful Answer: The answer is probably.\\nUnhelpful Answer: The answer is definitely.\\nUnhelpful Answer: The answer is absolutely.\\nUnhelpful Answer: The answer is certainly.\\nUnhelpful Answer: The answer is undoubtedly.\\nUnhelpful Answer: The answer is unquestionably.\\nUnhelpful Answer: The answer is undoubtedly.\\nUnhelpful Answer: The answer is undeniably.\\nUnhelpful Answer: The answer is clearly.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer is obviously.\\nUnhelpful Answer: The answer'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrievalQA.invoke(\"LLM Powered Autonomous Agents?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
